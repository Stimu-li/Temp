# Exercise 11: Hadoop Single Node Cluster Installation

**Date:** _______________

**Aim:** To install, configure, and run Hadoop and HDFS on a single node cluster.

## Algorithm

1. **System Preparation:**
   - Update Ubuntu system packages
   - Install Java Development Kit (JDK 8)
   - Configure Java environment variables
   - Verify Java installation

2. **User and SSH Configuration:**
   - Create dedicated Hadoop user and group
   - Install and configure OpenSSH server
   - Generate SSH key pairs for passwordless authentication
   - Configure SSH access for localhost

3. **Hadoop Installation:**
   - Download Hadoop distribution package
   - Extract and install Hadoop in system directory
   - Create symbolic links for easy access
   - Set proper file ownership and permissions

4. **Environment Configuration:**
   - Configure Hadoop environment variables
   - Set up JAVA_HOME in Hadoop configuration
   - Configure core Hadoop XML files
   - Create necessary directory structures

5. **HDFS Setup:**
   - Format the Hadoop Distributed File System
   - Configure namenode and datanode directories
   - Initialize file system metadata

6. **Service Management:**
   - Start Hadoop services (DFS and YARN)
   - Verify service status and functionality
   - Test web interfaces for monitoring

## Concept

- **Hadoop:** Open-source framework for distributed storage and processing of large datasets
- **HDFS (Hadoop Distributed File System):** Distributed file system designed for storing large files
- **Single Node Cluster:** Hadoop installation on one machine for development and testing
- **NameNode:** Master node that manages file system metadata
- **DataNode:** Worker nodes that store actual data blocks
- **YARN (Yet Another Resource Negotiator):** Resource management and job scheduling framework

## Detailed Installation Procedure

### Step 1: System Update and Java Installation

```bash
# Update system packages
ubuntu@ubuntu:~$ sudo apt update
ubuntu@ubuntu:~$ sudo apt upgrade -y

# Install OpenJDK 8
ubuntu@ubuntu:~$ sudo apt install openjdk-8-jdk

# Verify Java installation
ubuntu@ubuntu:~$ java -version
openjdk version "1.8.0_342"
OpenJDK Runtime Environment (build 1.8.0_342-8u342-b07-0ubuntu1~20.04-b07)
OpenJDK 64-Bit Server VM (build 25.342-b07, mixed mode)

ubuntu@ubuntu:~$ javac -version
javac 1.8.0_342
```

### Step 2: Java Environment Configuration

```bash
# Find Java installation path
ubuntu@ubuntu:~$ which javac
/usr/bin/javac

ubuntu@ubuntu:~$ readlink -f /usr/bin/javac
/usr/lib/jvm/java-8-openjdk-amd64/bin/javac
```

### Step 3: Create Hadoop User and Group

```bash
# Create Hadoop group
ubuntu@ubuntu:~$ sudo addgroup hadoopgroup
Adding group `hadoopgroup' (GID 1001) ...
Done.

# Create Hadoop user
ubuntu@ubuntu:~$ sudo adduser --ingroup hadoopgroup hduser
Adding user `hduser' ...
Adding new user `hduser' (1001) with group `hadoopgroup' ...
Creating home directory `/home/hduser' ...
Copying files from `/etc/skel' ...
New password: cse123
Retype new password: cse123
passwd: password updated successfully
Changing the user information for hduser
Enter the new value, or press ENTER for the default
    Full Name []: Hadoop User
    Room Number []:
    Work Phone []:
    Home Phone []:
    Other []:
Is the information correct? [Y/n] y
```

### Step 4: SSH Configuration

```bash
# Install OpenSSH server and client
ubuntu@ubuntu:~$ sudo apt install openssh-server openssh-client -y

# Switch to hduser account
ubuntu@ubuntu:~$ su - hduser
Password: cse123

# Generate SSH key pair
hduser@ubuntu:~$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
Generating public/private rsa key pair.
Your identification has been saved in /home/hduser/.ssh/id_rsa.
Your public key has been saved in /home/hduser/.ssh/id_rsa.pub.

# Add public key to authorized keys
hduser@ubuntu:~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Set proper permissions
hduser@ubuntu:~$ chmod 0600 ~/.ssh/authorized_keys

# Verify SSH directory contents
hduser@ubuntu:~$ cd .ssh/
hduser@ubuntu:~/.ssh$ ls -l
total 12
-rw------- 1 hduser hadoopgroup 567 Oct 14 10:03 authorized_keys
-rw------- 1 hduser hadoopgroup 2602 Oct 14 10:03 id_rsa
-rw-r--r-- 1 hduser hadoopgroup 567 Oct 14 10:03 id_rsa.pub

# Test passwordless SSH
hduser@ubuntu:~$ ssh localhost
# Should login without password prompt
```

### Step 5: Hadoop Installation

```bash
# Download Hadoop distribution
hduser@ubuntu:~$ wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

# Extract Hadoop archive
hduser@ubuntu:~$ tar xzf hadoop-3.3.4.tar.gz

# Switch to admin user to move files
hduser@ubuntu:~$ su - ubuntu
Password: [ubuntu_password]

# Move Hadoop to system directory
ubuntu@ubuntu:~$ sudo mv /home/hduser/hadoop-3.3.4 /usr/local/

# Create symbolic link
ubuntu@ubuntu:~$ sudo ln -sf /usr/local/hadoop-3.3.4/ /usr/local/hadoop

# Change ownership to hduser
ubuntu@ubuntu:~$ sudo chown -R hduser:hadoopgroup /usr/local/hadoop*
```

### Step 6: Environment Variable Configuration

```bash
# Switch back to hduser
ubuntu@ubuntu:~$ su - hduser
Password: cse123

# Edit .bashrc file
hduser@ubuntu:~$ nano ~/.bashrc

# Add the following lines at the end:
# Hadoop configuration
export HADOOP_PREFIX=/usr/local/hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

# Native path
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib/native"

# Java path
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# OS path
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Apply changes
hduser@ubuntu:~$ source ~/.bashrc
```

### Step 7: Hadoop Environment Configuration

```bash
# Navigate to Hadoop configuration directory
hduser@ubuntu:~$ cd /usr/local/hadoop/etc/hadoop/

# Edit hadoop-env.sh
hduser@ubuntu:/usr/local/hadoop/etc/hadoop$ nano hadoop-env.sh

# Add Java home at the end:
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
```

### Step 8: Core Hadoop Configuration Files

#### Configure core-site.xml

```bash
hduser@ubuntu:/usr/local/hadoop/etc/hadoop$ nano core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

#### Configure hdfs-site.xml

```bash
hduser@ubuntu:/usr/local/hadoop/etc/hadoop$ nano hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>
    </property>
</configuration>
```

#### Configure mapred-site.xml

```bash
hduser@ubuntu:/usr/local/hadoop/etc/hadoop$ nano mapred-site.xml
```

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

#### Configure yarn-site.xml

```bash
hduser@ubuntu:/usr/local/hadoop/etc/hadoop$ nano yarn-site.xml
```

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>127.0.0.1</value>
    </property>
    <property>
        <name>yarn.acl.enable</name>
        <value>0</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

### Step 9: Create HDFS Directory Structure

```bash
# Create Hadoop data directories
hduser@ubuntu:~$ cd /usr/local/hadoop
hduser@ubuntu:/usr/local/hadoop$ mkdir -p hadoopdata/hdfs/namenode
hduser@ubuntu:/usr/local/hadoop$ mkdir -p hadoopdata/hdfs/datanode

# Verify directory structure
hduser@ubuntu:/usr/local/hadoop$ ls -la hadoopdata/hdfs/
total 16
drwxrwxr-x 4 hduser hadoopgroup 4096 Oct 14 11:30 .
drwxrwxr-x 3 hduser hadoopgroup 4096 Oct 14 11:30 ..
drwxrwxr-x 2 hduser hadoopgroup 4096 Oct 14 11:30 datanode
drwxrwxr-x 2 hduser hadoopgroup 4096 Oct 14 11:30 namenode
```

### Step 10: Format HDFS Namenode

```bash
# Format the namenode
hduser@ubuntu:/usr/local/hadoop$ hdfs namenode -format
WARNING: /usr/local/hadoop/logs does not exist. Creating.
2023-10-14 11:35:03,594 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG: host = ubuntu/127.0.1.1
STARTUP_MSG: args = [-format]
STARTUP_MSG: version = 3.3.4
...
2023-10-14 11:35:04,967 INFO common.Storage: Storage directory
/usr/local/hadoop/hadoopdata/hdfs/namenode has been successfully formatted.
...
2023-10-14 11:35:05,172 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
```

### Step 11: Start Hadoop Services

```bash
# Start HDFS services
hduser@ubuntu:~$ start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [ubuntu]

# Verify HDFS services
hduser@ubuntu:~$ jps
57464 NameNode
57610 DataNode
57824 SecondaryNameNode
57982 Jps

# Start YARN services
hduser@ubuntu:~$ start-yarn.sh
Starting resourcemanager
Starting nodemanagers

# Verify all services
hduser@ubuntu:~$ jps
57464 NameNode
57610 DataNode
57824 SecondaryNameNode
58082 ResourceManager
58221 NodeManager
58374 Jps
```

### Alternative: Start All Services

```bash
# Start all Hadoop services at once
hduser@ubuntu:~$ start-all.sh

# Verify Hadoop version
hduser@ubuntu:~$ hadoop version
Hadoop 3.3.4
Source code repository https://github.com/apache/hadoop.git -r a585763716a3345d3e6412996f18013e5d4f8a1c
Compiled by stevel on 2022-07-29T12:32Z
Compiled with protoc 3.7.1
From source with checksum fb9dd8918a7d8a7dd4da4c546ee25449
This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.4.jar
```

### Step 12: Verify Installation

#### Web Interface Access

- **NameNode Web UI:** <http://localhost:9870>
- **DataNode Web UI:** <http://localhost:9864>
- **ResourceManager Web UI:** <http://localhost:8088>

#### Command Line Verification

```bash
# Check HDFS status
hduser@ubuntu:~$ hdfs dfsadmin -report
Configured Capacity: 123456789012 (115.02 GB)
Present Capacity: 120987654321 (112.72 GB)
DFS Remaining: 120987654321 (112.72 GB)
DFS Used: 24576 (24 KB)
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0

# Test HDFS operations
hduser@ubuntu:~$ hdfs dfs -mkdir /test
hduser@ubuntu:~$ hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2023-10-14 11:45 /test

# Create a test file
hduser@ubuntu:~$ echo "Hello Hadoop!" > test.txt
hduser@ubuntu:~$ hdfs dfs -put test.txt /test/
hduser@ubuntu:~$ hdfs dfs -cat /test/test.txt
Hello Hadoop!
```

### Step 13: Stop Services

```bash
# Stop YARN services
hduser@ubuntu:~$ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager

# Stop HDFS services
hduser@ubuntu:~$ stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [ubuntu]

# Alternative: Stop all services
hduser@ubuntu:~$ stop-all.sh
```

## Troubleshooting Common Issues

### Permission Denied Errors

```bash
sudo chown -R hduser:hadoopgroup /usr/local/hadoop*
sudo chmod -R 755 /usr/local/hadoop
```

### Java Home Issues

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
```

### SSH Connection Issues

```bash
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
```

### Port Already in Use

```bash
# Check running processes
sudo netstat -tulpn | grep :9000
# Kill conflicting processes if needed
```

## HDFS Basic Operations

### File Operations

```bash
# Create directory in HDFS
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hduser

# Upload file to HDFS
hdfs dfs -put localfile.txt /user/hduser/

# Download file from HDFS
hdfs dfs -get /user/hduser/localfile.txt

# List files in HDFS
hdfs dfs -ls /user/hduser

# View file contents
hdfs dfs -cat /user/hduser/localfile.txt

# Copy files within HDFS
hdfs dfs -cp /user/hduser/file1.txt /user/hduser/file2.txt

# Move files within HDFS
hdfs dfs -mv /user/hduser/file1.txt /user/hduser/backup/

# Delete files from HDFS
hdfs dfs -rm /user/hduser/file1.txt

# Delete directory recursively
hdfs dfs -rm -r /user/hduser/backup
```

### Administrative Commands

```bash
# Check HDFS health
hdfs fsck /

# Report HDFS status
hdfs dfsadmin -report

# Safe mode operations
hdfs dfsadmin -safemode get
hdfs dfsadmin -safemode leave

# Refresh namenode
hdfs dfsadmin -refreshNodes
```

## Performance Tuning Tips

### Memory Configuration

```xml
<!-- In yarn-site.xml -->
<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>8192</value>
</property>

<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8192</value>
</property>
```

### JVM Settings

```bash
# In hadoop-env.sh
export HADOOP_HEAPSIZE=2048
export HADOOP_NAMENODE_OPTS="-Xmx2048m"
export HADOOP_DATANODE_OPTS="-Xmx1024m"
```

## Security Considerations

### File Permissions

```bash
# Set appropriate permissions
hdfs dfs -chmod 755 /user/hduser
hdfs dfs -chown hduser:hadoopgroup /user/hduser
```

### Network Security

- Configure firewall rules for Hadoop ports
- Use Kerberos for authentication in production
- Enable SSL/TLS for web interfaces

## Best Practices

1. **Regular Backups:** Schedule regular HDFS backups
2. **Monitoring:** Set up monitoring for cluster health
3. **Resource Management:** Configure appropriate memory and CPU limits
4. **Log Management:** Implement log rotation and archiving
5. **Security Updates:** Keep Hadoop and Java updated

## Result

Thus the installation and configuration of Hadoop and HDFS single node cluster was executed successfully.

---

**Exercise completed:** _______________