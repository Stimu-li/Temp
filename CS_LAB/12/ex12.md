# Exercise 12: Word Count using MapReduce

**Date:** _______________

**Aim:** To implement a Word Count program using MapReduce to demonstrate Map and Reduce tasks.

## Algorithm

1. **Input Analysis:**
   - Read input text file containing words and sentences
   - Parse text into individual tokens (words)
   - Prepare data for distributed processing

2. **Map Phase:**
   - Split input text into words using StringTokenizer
   - Emit each word as key with count of 1 as value
   - Create (word, 1) pairs for all words

3. **Shuffle and Sort:**
   - Group all values by key (word)
   - Sort keys alphabetically
   - Prepare data for reduce phase

4. **Reduce Phase:**
   - Sum all values for each unique key (word)
   - Output final count for each word
   - Write results to output file

5. **Execution:**
   - Package code as JAR file
   - Submit job to Hadoop cluster
   - Monitor execution and collect results

## Concept

- **MapReduce:** Programming model for processing large datasets in distributed computing
- **Mapper:** Processes input and produces intermediate key-value pairs
- **Reducer:** Processes intermediate data and produces final output
- **Combiner:** Local reducer that optimizes data transfer between map and reduce
- **HDFS Integration:** Reads from and writes to Hadoop Distributed File System
- **Job Configuration:** Defines input/output paths and processing classes

## Java Implementation

```java
package WordCount;

import java.io.*;
import java.util.*;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.*;
import org.apache.hadoop.mapreduce.lib.output.*;

public class WordCount {
    
    // Mapper class
    public static class TokenizerMapper 
        extends Mapper<Object, Text, Text, IntWritable> {
        
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        
        public void map(Object key, Text value, Context context) 
            throws IOException, InterruptedException {
            
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken().toLowerCase());
                context.write(word, one);
            }
        }
    }
    
    // Reducer class
    public static class IntSumReducer 
        extends Reducer<Text, IntWritable, Text, IntWritable> {
        
        private IntWritable result = new IntWritable();
        
        public void reduce(Text key, Iterable<IntWritable> values, 
            Context context) throws IOException, InterruptedException {
            
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }
    
    // Driver method
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

## Detailed Procedure

### Step 1: Eclipse Project Setup

1. Open Eclipse IDE
2. Create new Java Project: File → New → Java Project
3. Project name: `WordCount`
4. Select JavaSE-1.8 and click Finish
5. Right-click on project → Properties → Libraries → Add External JARs
6. Add the following JAR files from `/usr/local/hadoop/share/hadoop/`:
   - `common/hadoop-common-3.3.4.jar`
   - `common/lib/common-cli-1.2.jar`
   - `mapreduce/hadoop-mapreduce-client-core-3.3.4.jar`

### Step 2: Package and Class Creation

```bash
# Create package structure
Right-click on src → New → Package
Package name: WordCount

# Create Java class
Right-click on package → New → Class
Class name: WordCount.java
```

### Step 3: JAR File Creation

1. Right-click on WordCount project → Export
2. Select JAR file under Java
3. Choose export destination (e.g., `/home/hduser/wordcount.jar`)
4. Click Finish

### Step 4: Input File Preparation

Create `input.txt` file:

```text
hi how are you
welcome to velammal engineering college
we are from computer science department
today we are gonna learn about mapreduce concept
then we will apply the mapreduce programming knowledge to process the text file
the output printed should be the count of occurrences of each word in the text file
here ends our today program so bye
```

### Step 5: HDFS Operations

```bash
# Start Hadoop services
hduser@ubuntu:~$ start-dfs.sh
hduser@ubuntu:~$ start-yarn.sh

# Verify services
hduser@ubuntu:~$ jps
11147 NameNode
11300 DataNode
11485 SecondaryNameNode
11638 ResourceManager
11949 NodeManager
12138 Jps

# Create HDFS directories
hduser@ubuntu:~$ hdfs dfs -mkdir /WordCount
hduser@ubuntu:~$ hdfs dfs -mkdir /WordCount/Input

# Upload input file to HDFS
hduser@ubuntu:~$ hdfs dfs -put input.txt /WordCount/Input/

# Verify file upload
hduser@ubuntu:~$ hdfs dfs -ls /WordCount/Input/
Found 1 items
-rw-r--r--   1 hduser supergroup        344 2023-10-14 15:30 /WordCount/Input/input.txt
```

### Step 6: Execute MapReduce Job

```bash
# Run the WordCount program
hduser@ubuntu:~$ hadoop jar wordcount.jar WordCount.WordCount /WordCount/Input/input.txt /WordCount/Output

# Job execution output (simplified)
INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
INFO input.FileInputFormat: Total input paths to process : 1
INFO mapreduce.JobSubmitter: number of splits:1
INFO mapreduce.Job: Running job: job_1697280324_0001
INFO mapreduce.Job: map 0% reduce 0%
INFO mapreduce.Job: map 100% reduce 0%
INFO mapreduce.Job: map 100% reduce 100%
INFO mapreduce.Job: Job job_1697280324_0001 completed successfully

Job Counters:
    Map input records=8
    Map output records=59
    Reduce input records=48
    Reduce output records=48
    
File System Counters:
    HDFS: Number of bytes read=456
    HDFS: Number of bytes written=390
```

### Step 7: View Results

```bash
# Check output directory
hduser@ubuntu:~$ hdfs dfs -ls /WordCount/Output/
Found 2 items
-rw-r--r--   1 hduser supergroup          0 2023-10-14 15:35 /WordCount/Output/_SUCCESS
-rw-r--r--   1 hduser supergroup        390 2023-10-14 15:35 /WordCount/Output/part-r-00000

# View word count results
hduser@ubuntu:~$ hdfs dfs -cat /WordCount/Output/part-r-00000
```

## Sample Output

```text
about       1
apply       1
are         3
bye         1
college     1
computer    1
concept     1
count       1
department  1
each        2
engineering 1
file        2
from        1
gonna       1
here        1
hi          1
how         1
knowledge   1
learn       1
mapreduce   2
occurrences 1
of          2
our         1
output      1
printed     1
process     1
program     1
programming 1
science     1
should      1
so          1
text        2
the         4
to          2
today       2
velammal    1
we          2
welcome     1
will        1
word        1
you         1
```

## MapReduce Workflow Explanation

1. **Input Splitting:** Hadoop splits input file into chunks for parallel processing
2. **Map Phase:** Each mapper processes its chunk and emits (word, 1) pairs
3. **Shuffle:** Framework groups all values by key (word)
4. **Reduce Phase:** Reducer sums up counts for each unique word
5. **Output:** Final results written to HDFS output directory

## Key Components

- **TokenizerMapper:** Splits text into words and emits (word, 1) pairs
- **IntSumReducer:** Aggregates counts for each word
- **Job Configuration:** Sets up input/output paths and processing classes
- **Combiner:** Uses same reducer logic to optimize network traffic

## Performance Optimization

```java
// Enable compression
conf.setBoolean("mapred.compress.map.output", true);
conf.setClass("mapred.map.output.compression.codec", 
              GzipCodec.class, CompressionCodec.class);

// Set number of reducers
job.setNumReduceTasks(2);
```

## Troubleshooting Common Issues

1. **ClassNotFoundException:** Ensure all Hadoop JARs are in classpath
2. **Permission Denied:** Check HDFS directory permissions
3. **Job Failure:** Verify input file exists and output directory doesn't exist
4. **Memory Issues:** Adjust JVM heap size in mapred-site.xml

## Alternative Input/Output Formats

```java
// For processing multiple small files
job.setInputFormatClass(CombineTextInputFormat.class);

// For custom output format
job.setOutputFormatClass(TextOutputFormat.class);
```

## Advanced MapReduce Concepts

### Custom Partitioner

```java
public static class WordPartitioner extends Partitioner<Text, IntWritable> {
    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        return (key.hashCode() & Integer.MAX_VALUE) % numPartitions;
    }
}
```

### Custom WritableComparable

```java
public class WordWritable implements WritableComparable<WordWritable> {
    private String word;
    private int count;
    
    // Implementation of Writable and Comparable methods
}
```

### Multiple Input Formats

```java
MultipleInputs.addInputPath(job, new Path("input1"), TextInputFormat.class, TokenizerMapper.class);
MultipleInputs.addInputPath(job, new Path("input2"), SequenceFileInputFormat.class, TokenizerMapper.class);
```

## Best Practices

1. **Use Combiners:** Reduce network traffic between map and reduce phases
2. **Optimal Block Size:** Configure appropriate HDFS block size for input files
3. **Memory Management:** Set appropriate heap sizes for mappers and reducers
4. **Data Locality:** Ensure mappers run on nodes containing input data
5. **Error Handling:** Implement proper exception handling in map and reduce methods

## Monitoring and Debugging

### Web Interface Monitoring

- **Job Tracker:** <http://localhost:8088> (YARN ResourceManager)
- **Name Node:** <http://localhost:9870> (HDFS NameNode)
- **Data Node:** <http://localhost:9864> (HDFS DataNode)

### Command Line Monitoring

```bash
# Check job status
yarn application -list

# View job logs
yarn logs -applicationId application_1234567890_0001

# Monitor HDFS usage
hdfs dfsadmin -report
```

## Result

Thus the Word Count program using MapReduce to demonstrate Map and Reduce tasks was implemented successfully.

---

**Exercise completed:** _______________